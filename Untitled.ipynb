{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance / Covariance / Expect Value \n",
    "\n",
    "$$ V(Y) = V(E[Y|X]) + E(V[Y|X]) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ratio Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate Parameter\n",
    "\n",
    "Assume $x_i$ are i.i.d. $\\sim f(X|\\theta)$ \n",
    "which is some distribution with unkown $\\theta \\in R^d$, the goal is figuring out how to estimate $\\theta$.\n",
    "\n",
    "### Method\n",
    "\n",
    "- ### Method of Memoments\n",
    "> Knowing $E[x_i] = g(\\theta)$ is some function of $\\theta$.\n",
    ">\n",
    "> By Law of Large Number ( the sample moments are \"close\" to population moments if n is large )\n",
    ">\n",
    "> $$P( |\\bar x - E[x_i]| > \\delta ) \\to 0  \\mbox{ as }  n \\to \\infty $$\n",
    ">\n",
    "> Therfore we can use $\\bar x$ to estimate $E[x_i]$ and thus estimate $g(\\theta)$.\n",
    ">\n",
    "##### Steps: \n",
    ">\n",
    "> - express population moments in terms of parameters\n",
    ">\n",
    "> - Replace population moments with sample moments\n",
    ">\n",
    "> - solve the parameters in terms of sample moments\n",
    "\n",
    "- ### Maximum Likelihood Estimation\n",
    "> Since $x_i \\sim $ iid $f(x|\\theta)$, considering the joint density function\n",
    ">\n",
    "> $$ f(x_1 ,\\dots, x_n | \\theta) = f(x_1|\\theta)f(x_2|\\theta) \\dots f(x_n|\\theta) $$\n",
    ">\n",
    "> We want to maximize the likelihood function\n",
    "> $$ \\max_\\theta \\{ f(x_1|\\theta)f(x_2|\\theta) \\dots f(x_n|\\theta)  \\}$$\n",
    "> Turned out it's samse as maximize the log-likelihood function\n",
    "> $$ \\max_\\theta \\{ \\log [f(x_1|\\theta)f(x_2|\\theta) \\dots f(x_n|\\theta) ] \\} = \\max_\\theta \\{ \\sum^n_i \\log f(x_i|\\theta) \\} $$\n",
    ">\n",
    ">\n",
    "> ###### Because log is monotonically increasing function, the critical point is golbal maximum \n",
    ">\n",
    "> Lemma : if $\\hat \\theta $ is the MLE of $\\theta$. The asymptotic variance of $\\hat \\theta $ is given by ${1\\over n I(\\theta_0) }$ ( Suppose $\\theta = \\theta_0$ ) \n",
    ">\n",
    "> Thm: Unver certain regularity and smoothness assumptions\n",
    ">\n",
    "> $$ \\sqrt{ n I(\\theta_0) } ( \\hat\\theta_n - \\theta_0 ) \\sim N(0,1) \\mbox{ as } n \\to \\infty $$\n",
    ">\n",
    "> ###### Therefore, we say MLEs are asymtotically normal and asymtotically unbiased\n",
    ">\n",
    "\n",
    "### Desirable properties of estimators\n",
    "\n",
    "- ### Ubiasedness\n",
    "\n",
    "- ### Small Variance\n",
    "\n",
    "- ### Consistency\n",
    "> $$ P( | \\hat\\theta_n - \\theta | > \\delta ) \\to 0 \\mbox{ as } n \\to \\infty $$\n",
    "\n",
    "- #### Regularity Condition\n",
    "> - the support of $f(x|\\theta)$ should not depends on $\\theta$\n",
    ">\n",
    "> - the density of $f$ is twice continiously differatiable\n",
    ">\n",
    "> #### We therefore claimed under the regularity condition, MLEs are consistent, asymptotically normal and asymtotically unbiased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian inference\n",
    "\n",
    "- #### Precision (textbook p.290)\n",
    "> Look at normal distribution, replacing $\\sigma^2$ with $\\xi$.\n",
    "> $$\\sigma^2 = {1\\over \\xi }$$\n",
    "> $\\xi$ is called Precision\n",
    ">\n",
    "> ##### Let's consider the case of unknown Mean and Kown Variance\n",
    "> Knowing that the variance is $\\xi_0$ and $\\theta$ is unknown with the prior distribution $N(\\theta_0, \\xi_{pr} ) $\n",
    ">\n",
    ">\n",
    "> $x_i$ iid $\\sim N(\\theta, \\xi_0 ) $ \n",
    ">\n",
    "> $\\theta$ $\\sim N(\\theta_0, \\xi_{pr} ) $\n",
    ">> Thus, the posterior distribution is\n",
    ">> $$ \\propto \\exp(-{1\\over 2} Q(\\theta) ) $$ where $Q(\\theta)$ is a quadratic polynomial.\n",
    ">>\n",
    ">> ##### It turns out that the posterior density is normal with posterior mean $\\theta_{post}$ and $\\xi_{post}$. Note that the terms that do not depend on $\\theta$ do not affect the shape of the posterior density and are absorbed in the normalization constant that makes the posterior density integrate to 1.\n",
    ">>\n",
    ">> Thus, \n",
    ">> $$ \\theta_{post} = \\bar x ( { n \\xi_0 \\over n \\xi_0 + \\xi_{pr}} ) + \\theta_0 ( { \\xi_{prior} \\over n\\xi_0 + \\xi_{pr} } ) $$\n",
    ">>\n",
    ">> One way to explain it is the weight of the $\\theta_0$ and $\\bar x$\n",
    ">>\n",
    ">> $$ \\xi_{post} = n\\xi_0 + \\xi_{prior} $$\n",
    ">>\n",
    ">> It turns out that when n is large, n will dominate the function, thus $\\theta_{post} \\approx \\bar x $ and $\\xi_{post} \\approx n\\xi_0 $ which is just the variance of $\\bar x$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sufficiency / Efficiency (p. 21)\n",
    "\n",
    "- ### Sufficiency\n",
    "> A statistic $T(x_1,\\dots x_n) $ is said to be sufficient for $\\theta$ if the conditional distribution of $x_1,\\dots x_n$, given $T = t$, doesn't depend on $\\theta$ for any value of t\n",
    "> $$ P(x_1,\\dots | T) = \\mbox{ a function doesn't depend on } \\theta $$\n",
    ">\n",
    "> ##### Factorization Thm:\n",
    "> ##### A neccessary and sufficient condition for $T(x_1,\\dots x_n)$ to be sufficient for a parameter $\\theta$ is that the joint probability function can be converted into \n",
    "> $$ f(x_1,\\dots x_n | \\theta) = g(T(x_1,\\dots x_n), \\theta) h(x_1,\\dots x_n) $$\n",
    ">> ##### 1. This implies that if a sufficient statistic exists, the MLE is just a function of that sufficient statistic, since after getting data, $h(x_1,\\dots x_n) $ is just a constant number, thus we only need to maximize the term $ g(T(x_1,\\dots x_n), \\theta)$\n",
    ">>\n",
    ">> ##### 2. Sufficient statistics can be used to help reduce cariance of an estimator ( useful for unbiased est )\n",
    ">>\n",
    ">> ##### 3. The posterior distribution depends on data only through T ( Bayesian )\n",
    "\n",
    "> ##### Rao-Blackwell Theorem\n",
    "> ###### if an estimator is not a function of a sufficient statistic, it can be improved.\n",
    ">\n",
    "> Let  $ \\tilde\\theta = E(\\hat\\theta | T )$, and $\\hat \\theta$ is an estimator of $\\theta$. Thus\n",
    "> $$ E [( \\tilde\\theta - \\theta )^2] \\leq E[(\\hat\\theta - \\theta)^2 ]$$\n",
    "\n",
    "- ### Efficiency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
