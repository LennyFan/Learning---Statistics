{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance / Covariance / Expect Value \n",
    "\n",
    "[ to be filled ]\n",
    "\n",
    "$$ V(Y) = V(E[Y|X]) + E(V[Y|X]) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution table\n",
    "\n",
    "- Chi-square Distribution [ 1 ]\n",
    "> $Z \\sim N(0,1)$, $Z^2$ is said to have $\\chi^2_1$ distribution with 1 degree of freedom\n",
    "> \n",
    "> Let $Y = Z^2$ \n",
    "> $$ P(Z^2 \\geq y) = \\alpha $$\n",
    ">\n",
    "> $$ P(Z^2 \\geq y) = P(Z\\geq \\sqrt{y}) + P(Z\\leq -\\sqrt{y}) = \\alpha$$ \n",
    ">\n",
    "> $$ \\therefore P(Z \\geq \\sqrt{y} ) = \\alpha/2 $$\n",
    "> \n",
    "> We can also use the above property to solve the density function of $\\chi^2_1$ [ to be filled ]\n",
    "\n",
    "- Chi-square Distribution [ degree of freedom ]\n",
    "> $Z_1 \\dots Z_n $ are iid $N(0,1)$\n",
    ">\n",
    "> then $Z_1^2 + \\dots Z^2_n $ has a $\\chi^2_n$ with n degrees of freedom\n",
    ">\n",
    "> ###### Corollary:\n",
    ">\n",
    "> If $x_1 \\dots x_n$ are iid $N(\\mu, \\sigma^2)$\n",
    "> \n",
    "> $$ {S^2(n-1) \\over \\sigma^2 } \\mbox{ has a } \\chi^2_{n-1} \\mbox{ with n-1 degrees of freedom } $$\n",
    "> However, why we need $\\chi^2$ distribution is that we don't know the $\\sigma^2$\n",
    ">\n",
    "> Thus, we are going to introduce Student's t-distribution\n",
    "\n",
    "- Student's t-distribution ( t distribution )\n",
    "> Let Z to be standard normal, and W be a indepenednt $\\chi^2_m$, \n",
    ">\n",
    "> then ${Z\\over \\sqrt{ W/m } }\\sim $ t-distribution with m degrees of freedom\n",
    ">\n",
    "> Using the corollary above, we will get\n",
    "> $$ {\\bar x - \\mu \\over s/\\sqrt{n}} \\sim t_{n-1} $$\n",
    "\n",
    "- F distribution\n",
    "> Let $V \\sim \\chi^2_n$ , $U \\sim \\chi^2_m$ and U, V are independent\n",
    ">\n",
    "> $$ {U / m \\over V/n } \\sim F_{m,n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ratio Estimation\n",
    "\n",
    "[ to be filled ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate Parameter\n",
    "\n",
    "Assume $x_i$ are i.i.d. $\\sim f(X|\\theta)$ \n",
    "which is some distribution with unkown $\\theta \\in R^d$, the goal is figuring out how to estimate $\\theta$.\n",
    "\n",
    "### Method\n",
    "\n",
    "- ### Method of Memoments\n",
    "> Knowing $E[x_i] = g(\\theta)$ is some function of $\\theta$.\n",
    ">\n",
    "> By Law of Large Number ( the sample moments are \"close\" to population moments if n is large )\n",
    ">\n",
    "> $$P( |\\bar x - E[x_i]| > \\delta ) \\to 0  \\mbox{ as }  n \\to \\infty $$\n",
    ">\n",
    "> Therfore we can use $\\bar x$ to estimate $E[x_i]$ and thus estimate $g(\\theta)$.\n",
    ">\n",
    "##### Steps: \n",
    ">\n",
    "> - express population moments in terms of parameters\n",
    ">\n",
    "> - Replace population moments with sample moments\n",
    ">\n",
    "> - solve the parameters in terms of sample moments\n",
    "\n",
    "- ### Maximum Likelihood Estimation\n",
    "> Since $x_i \\sim $ iid $f(x|\\theta)$, considering the joint density function\n",
    ">\n",
    "> $$ f(x_1 ,\\dots, x_n | \\theta) = f(x_1|\\theta)f(x_2|\\theta) \\dots f(x_n|\\theta) $$\n",
    ">\n",
    "> We want to maximize the likelihood function\n",
    "> $$ \\max_\\theta \\{ f(x_1|\\theta)f(x_2|\\theta) \\dots f(x_n|\\theta)  \\}$$\n",
    "> Turned out it's samse as maximize the log-likelihood function\n",
    "> $$ \\max_\\theta \\{ \\log [f(x_1|\\theta)f(x_2|\\theta) \\dots f(x_n|\\theta) ] \\} = \\max_\\theta \\{ \\sum^n_i \\log f(x_i|\\theta) \\} $$\n",
    ">\n",
    ">\n",
    "> ###### Because log is monotonically increasing function, the critical point is golbal maximum \n",
    ">\n",
    "> Lemma : if $\\hat \\theta $ is the MLE of $\\theta$. The asymptotic variance of $\\hat \\theta $ is given by ${1\\over n I(\\theta_0) }$ ( Suppose $\\theta = \\theta_0$ ) \n",
    ">\n",
    "> Thm: Unver certain regularity and smoothness assumptions\n",
    ">\n",
    "> $$ \\sqrt{ n I(\\theta_0) } ( \\hat\\theta_n - \\theta_0 ) \\sim N(0,1) \\mbox{ as } n \\to \\infty $$\n",
    ">\n",
    "> ###### Therefore, we say MLEs are asymtotically normal and asymtotically unbiased\n",
    ">\n",
    "\n",
    "### Desirable properties of estimators\n",
    "\n",
    "- ### Ubiasedness\n",
    "\n",
    "- ### Small Variance\n",
    "\n",
    "- ### Consistency\n",
    "> $$ P( | \\hat\\theta_n - \\theta | > \\delta ) \\to 0 \\mbox{ as } n \\to \\infty $$\n",
    "\n",
    "- #### Regularity Condition\n",
    "> - the support of $f(x|\\theta)$ should not depends on $\\theta$\n",
    ">\n",
    "> - the density of $f$ is twice continiously differatiable\n",
    ">\n",
    "> #### We therefore claimed under the regularity condition, MLEs are consistent, asymptotically normal and asymtotically unbiased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian inference\n",
    "\n",
    "- #### Precision (textbook p.290)\n",
    "> Look at normal distribution, replacing $\\sigma^2$ with $\\xi$.\n",
    "> $$\\sigma^2 = {1\\over \\xi }$$\n",
    "> $\\xi$ is called Precision\n",
    ">\n",
    "> ##### Let's consider the case of unknown Mean and Kown Variance\n",
    "> Knowing that the variance is $\\xi_0$ and $\\theta$ is unknown with the prior distribution $N(\\theta_0, \\xi_{pr} ) $\n",
    ">\n",
    ">\n",
    "> $x_i$ iid $\\sim N(\\theta, \\xi_0 ) $ \n",
    ">\n",
    "> $\\theta$ $\\sim N(\\theta_0, \\xi_{pr} ) $\n",
    ">> Thus, the posterior distribution is\n",
    ">> $$ \\propto \\exp(-{1\\over 2} Q(\\theta) ) $$ where $Q(\\theta)$ is a quadratic polynomial.\n",
    ">>\n",
    ">> ##### It turns out that the posterior density is normal with posterior mean $\\theta_{post}$ and $\\xi_{post}$. Note that the terms that do not depend on $\\theta$ do not affect the shape of the posterior density and are absorbed in the normalization constant that makes the posterior density integrate to 1.\n",
    ">>\n",
    ">> Thus, \n",
    ">> $$ \\theta_{post} = \\bar x ( { n \\xi_0 \\over n \\xi_0 + \\xi_{pr}} ) + \\theta_0 ( { \\xi_{prior} \\over n\\xi_0 + \\xi_{pr} } ) $$\n",
    ">>\n",
    ">> One way to explain it is the weight of the $\\theta_0$ and $\\bar x$\n",
    ">>\n",
    ">> $$ \\xi_{post} = n\\xi_0 + \\xi_{prior} $$\n",
    ">>\n",
    ">> It turns out that when n is large, n will dominate the function, thus $\\theta_{post} \\approx \\bar x $ and $\\xi_{post} \\approx n\\xi_0 $ which is just the variance of $\\bar x$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sufficiency / Efficiency (p. 21)\n",
    "\n",
    "- ### Sufficiency\n",
    "> A statistic $T(x_1,\\dots x_n) $ is said to be sufficient for $\\theta$ if the conditional distribution of $x_1,\\dots x_n$, given $T = t$, doesn't depend on $\\theta$ for any value of t\n",
    "> $$ P(x_1,\\dots | T) = \\mbox{ a function doesn't depend on } \\theta $$\n",
    ">\n",
    "> ##### Factorization Thm:\n",
    "> ##### A neccessary and sufficient condition for $T(x_1,\\dots x_n)$ to be sufficient for a parameter $\\theta$ is that the joint probability function can be converted into \n",
    "> $$ f(x_1,\\dots x_n | \\theta) = g(T(x_1,\\dots x_n), \\theta) h(x_1,\\dots x_n) $$\n",
    ">> ##### 1. This implies that if a sufficient statistic exists, the MLE is just a function of that sufficient statistic, since after getting data, $h(x_1,\\dots x_n) $ is just a constant number, thus we only need to maximize the term $ g(T(x_1,\\dots x_n), \\theta)$\n",
    ">>\n",
    ">> ##### 2. Sufficient statistics can be used to help reduce cariance of an estimator ( useful for unbiased est )\n",
    ">>\n",
    ">> ##### 3. The posterior distribution depends on data only through T ( Bayesian )\n",
    ">\n",
    "> ##### Rao-Blackwell Theorem\n",
    "> ###### if an estimator is not a function of a sufficient statistic, it can be improved.\n",
    ">\n",
    "> Let  $ \\tilde\\theta = E(\\hat\\theta | T )$, and $\\hat \\theta$ is an estimator of $\\theta$. Thus\n",
    "> $$ E [( \\tilde\\theta - \\theta )^2] \\leq E[(\\hat\\theta - \\theta)^2 ]$$\n",
    "\n",
    "- ### Efficiency\n",
    "> Given two estimates, $\\hat \\theta $ and $\\tilde\\theta$, of a parameter $\\theta$, the efficiency of $\\hat\\theta$ relative to $\\tilde\\theta$ is defined to be \n",
    "> $$ eff(\\hat\\theta, \\tilde\\theta) = {V(\\tilde\\theta)\\over V(\\hat\\theta) } $$\n",
    ">\n",
    "> ##### Cramer-Rao Inequality\n",
    "> Let $x_i$ to be iid with density function $f(x|\\theta)$. Let $T = t(x_1,\\dots,x_n)$ be an unbiased estimate of $\\theta$. Then, under smoothness assumptions on $f(x|\\theta)$, \n",
    "> $$ V(T) \\geq {1\\over n I(\\theta)} $$\n",
    ">> For an unbiased estimator, it is efficient if its variance achieves the C-R Lower Bound ( also for asymtotically unbiased estimators )\n",
    ">>\n",
    ">> Therefore, we can cocluded that since MLE is asymtotically unbiased, thus it's asymtotically efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing\n",
    "\n",
    "When we know the $\\sigma$, but don't know the $\\mu$\n",
    "\n",
    "- TYPE 1 ERROR : Reject $H_0$ when $H_0$ is True ( False Positive )\n",
    "> The probability of type 1 error is called the Significance level $\\alpha$:\n",
    "> $$ P(\\mbox{ type 1 error } ) = \\alpha $$\n",
    "- TYPE 2 ERROR : Accept $H_0$ when $H_0$ is Wrong ( Fasle Negative )\n",
    "> The probability of type 2 error is denoted by $\\beta$\n",
    ">\n",
    "> The power of test is $1-\\beta$\n",
    "> $$  P(\\mbox{ type 2 error } ) = \\beta $$\n",
    "\n",
    "> Turn out it's a trade-off between type 1 and type 2 error. \n",
    ">\n",
    "> ##### How might we design a \"good\" decisio rule ?\n",
    ">\n",
    "> Ans: likelihood ratio (LRT - likelihood ratio test)\n",
    ">\n",
    "> Neyman-Pearson Lemma asserts that the likelihood ratio is the most powerful\n",
    ">\n",
    "> ##### Neyman-Pearson Lemma: (text book p.332)\n",
    ">\n",
    "> [ to be filled ]\n",
    ">\n",
    ">\n",
    "> ##### Conclusion:\n",
    ">\n",
    "> The goal is finding a c and $\\alpha$ , s.t. \n",
    "> $$ P(\\Lambda \\leq c | H_0) \\leq \\alpha $$\n",
    "> where $\\Lambda$ is likelihood ratio\n",
    "> \n",
    "> After simplify the likelihood function, it turned out the inequaliy above can be written in thr form\n",
    ">\n",
    "> $$ P( {\\bar x - \\mu_0 \\over \\sigma/\\sqrt{n} }  \\leq {C(c) - \\mu_0 \\over\\sigma/\\sqrt{n} } | H_0 ) \\leq \\alpha $$\n",
    "> where $C(c)$ is a function of c\n",
    "> \n",
    "> ##### Therefore, if we define $\\alpha$ first, then we can calculate c by solving the equation above (vice versa). Actually, in this case we don't really even care the value of c\n",
    ">\n",
    "> ##### Defince $\\alpha$ first : We want to control Type 1 error in some Significance level \n",
    ">\n",
    "> ##### Defince c first : We have some belief in the prior probabiliy in $H_0$ and $H_a$.\n",
    ">\n",
    "> $$  { P(\\theta_0 | x_i \\dots ) \\over P(\\theta_a | x_i \\dots ) } = { P( x_i \\dots | \\theta_0) P(\\theta_0) \\over P( x_i \\dots | \\theta_a  )P(\\theta_a)}$$\n",
    ">\n",
    "> $$ { P( x_i \\dots | \\theta_0)  \\over P( x_i \\dots | \\theta_a  )} \\leq { P(\\theta_a) \\over P(\\theta_0) } = c$$\n",
    ">\n",
    ">\n",
    "\n",
    "\n",
    "\n",
    "- ### GLRT ( Generalized likelihood ratio tests )\n",
    "> Let $H = H_0 \\cup H_a$ where $H_0\\cap H_a = \\emptyset$\n",
    "> $$ GLRT = { \\max_{\\theta \\in H_0} f(x_i\\dots | \\theta )  \\over \\max_{\\theta \\in H} f(x_i\\dots | \\theta ) } \\in (0,1]$$\n",
    "> #### Note that the distribution of this statistic under $H_0$ is chi-square with 1 degree of freedom (uptail), but we can rewrite this relation in standard normal form (two tail with probability $\\alpha/2$  [ Check Distribution Table ]\n",
    ">\n",
    "> \n",
    ">\n",
    "> #### GLTR is uniformly most powerful statistic ( Wilk's Thm )\n",
    ">\n",
    ">\n",
    "\n",
    "\n",
    "> #### two sample test: (GLRT)\n",
    ">\n",
    "> Let's say $x_i$ iid $N(\\mu_1,\\sigma^2)$ and $y_i$ iid $N(\\mu_2, \\sigma^2)$. We want to test if $\\mu_2 = \\mu_1$.\n",
    ">\n",
    "> $$ H_0 : \\mu_1 = \\mu_2 \\;\\;\\;\\; H_a \\mu_1 \\neq \\mu_2 $$\n",
    ">\n",
    "> Since $x_i$ , $y_i$ are independent [ https://onlinecourses.science.psu.edu/stat414/node/172 ]\n",
    ">\n",
    "> $$ \\therefore z_i = x_i - y_i \\sim N(\\mu_1 - \\mu_2, 2 \\sigma^2 ) $$\n",
    "> \n",
    "> $$ \\mbox{ reject if } \\;\\;\\;\\; |{ \\bar z - 0 \\over \\sqrt{2/n} \\sigma } | > Z(\\alpha/2) $$\n",
    ">\n",
    "> ###### if $x_i$ and $y_i$ have different size ----------------------- ?\n",
    ">\n",
    "> $\\bar x \\sim N( \\mu_1, \\sigma^2/n_1 )$ , $\\bar y \\sim N(\\mu_2, \\sigma^2/n_2 ) $\n",
    ">\n",
    "> $$ \\therefore \\;\\; \\bar x - \\bar y \\sim N(\\mu_1 - \\mu_2, \\sigma^2 ( {1\\over n_1} + { 1 \\over n_2 } ) ) $$\n",
    ">\n",
    "> $$ \\mbox{ reject if } \\;\\;\\;\\; |{ \\bar x - \\bar y - 0 \\over \\sqrt{ {1\\over n_1} + { 1 \\over n_2 }} \\sigma } | > Z(\\alpha/2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How if we don't know the $\\sigma$ (p. 31)\n",
    "\n",
    "- Ans: T-distribution solves the problem [ Distribution Table ]\n",
    "\n",
    "> ##### One sample test (GLRT)\n",
    ">\n",
    "> $H_0: \\mu = \\mu_0 , \\sigma^2 > 0 $\n",
    ">\n",
    "> $H_a: \\mu \\neq \\mu_0 , \\sigma^2 > 0 $\n",
    ">\n",
    "> $$ GLRT : \\Lambda = {\\max_{\\mu,\\sigma^2 \\in H_0} f(x_i \\dots | \\mu, \\sigma^2 ) \\over \\max_{\\mu,\\sigma^2 \\in H} f(x_i \\dots | \\mu, \\sigma^2 ) } $$\n",
    ">\n",
    "> can be reduced to so-called t-test:\n",
    ">\n",
    "> $$ \\mu = \\bar x \\pm t_{n-1}(\\alpha/2) {s\\over \\sqrt{n}} $$\n",
    ">\n",
    "> the reject region is \n",
    "> $$ {\\bar x - \\mu_0 \\over s/\\sqrt{n}} > t_{n-1}(\\alpha/2) \\;\\; \\mbox{ and } \\;\\; {\\bar x - \\mu_0 \\over s/\\sqrt{n}} < t_{n-1}(\\alpha/2)$$\n",
    "\n",
    "\n",
    "> ##### Two sample test\n",
    ">\n",
    "> we want to find someway to estimate $\\sigma^2$, the goal is found out $s^2$ (pooled sample variance ), s.t. we can convert the function \n",
    ">\n",
    "> $$ \\mbox{ reject if } \\;\\;\\;\\; |{ \\bar x - \\bar y - 0 \\over \\sqrt{ {1\\over n_1} + { 1 \\over n_2 }} \\sigma } | > Z(\\alpha/2) $$\n",
    "> to \n",
    "> $$ \\mbox{ reject if } \\;\\;\\;\\; |{ \\bar x - \\bar y - 0 \\over \\sqrt{ {1\\over n_1} + { 1 \\over n_2 }} s } | > t_{n_1+n_2-2}(\\alpha/2) $$\n",
    ">\n",
    "> turn out that\n",
    "> $$ s^2 = {s_1^2 (n_1 - 1) + s_2^2 ( n_2-1) \\over n_1 + n_2 -2 } \\;\\;\\mbox{ ( weight distribution of s1 and s2 ) } $$\n",
    "> where\n",
    "> $$ s_1^2 =  {\\sum (x_i - \\bar x)^2\\over n_1 - 1} \\;\\;\\;,\\;\\;\\;\\;  s_2^2 =  {\\sum (x_i - \\bar x)^2\\over n_2 - 1} $$\n",
    ">\n",
    "> We can solve GLRT and get the same result as above ( text book p.427 )\n",
    ">\n",
    ">> if $\\sigma_1 \\neq \\sigma_2 $\n",
    ">>\n",
    ">> $$ \\mbox{ reject if } \\;\\;\\;\\; |{ \\bar x - \\bar y - 0 \\over \\sqrt{ {s_1^2\\over n_1} + { s_2^2 \\over n_2 } }} | > t_{r}(\\alpha/2) $$\n",
    ">> where r degree of freedom can be approximated by\n",
    ">> $$ r = { [s_1^2/n_1 + s_2^2/n_2 ]^2 \\over { ((s_1^2/n_1)^2 \\over n_1-1 } + { ((s_2^2/n_2)^2 \\over n_2-1 } } $$\n",
    ">>\n",
    ">> [ https://onlinecourses.science.psu.edu/stat414/node/275 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Conclusion of two sample test ( Power )\n",
    "> - The real difference( $|\\mu_x - \\mu_y|$ ) $ \\uparrow $ , power $ \\uparrow $\n",
    "> - $\\alpha$ $ \\uparrow $ , power $ \\uparrow $\n",
    "> - standar deviation $\\downarrow$ , power $ \\uparrow $\n",
    "> - sample size $ \\uparrow $,  power $ \\uparrow $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANOVA Test\n",
    "\n",
    "- One way ( only one factor ) [p.35]\n",
    "\n",
    "> SST = SSW + SSB\n",
    ">\n",
    "> Note that SSW $= \\sum^{n_i}_j (Y_{ij} - \\bar Y_i)^2 = (n_i-1)s_i^2 $\n",
    ">\n",
    "> Using F test, we get\n",
    ">\n",
    ">  $$ \\mbox{ reject if } \\;\\;\\;\\; { \\mbox{ SSB } / (k-1) \\over \\mbox{ SSW } / (n-k)} \\geq F_{k-1,n-k}(\\alpha) $$\n",
    ">\n",
    "> Note that we only reject upper tail, then we can use Tukey's test to see if any two of them are different from the other ( or use t-test again for each pairs )\n",
    ">\n",
    "> we has a so-called studentized range distribution with parameters I and I(J-1) ( assume all $n_i = J$ )\n",
    ">\n",
    "> ( Note the first degree freedom I means how many different populations/expirement )\n",
    ">\n",
    "> $$ \\mbox{ reject if } \\;\\;\\;\\; \n",
    "| \\bar Y_{i1}- \\bar Y_{i2} | \\geq q_{I,I(J-1)}(\\alpha){s_p\\over \\sqrt{J}} $$\n",
    "> where $s_p = SSW/(I(J-1)) $\n",
    "\n",
    "- Two way ( two factors ) [p.39]\n",
    "\n",
    "> $Y_{ijl} \\sim N(\\mu+\\alpha_i+\\beta_j + \\delta_{ij} , \\sigma^2 )$\n",
    ">\n",
    "> SST = SSW + SSA + SSB + SSAB\n",
    ">\n",
    "> There are three different hypothesis we can test\n",
    ">\n",
    "> $$ H_{0A} : \\; \\alpha_i = 0 \\forall i $$\n",
    ">\n",
    "> $$ H_{0B} : \\; \\beta_i = 0 \\forall i $$\n",
    ">\n",
    "> $$ H_{0AB} : \\; \\delta_{ij} = 0 \\forall i,j $$\n",
    ">\n",
    "> Same as oneway layout using F test\n",
    "> [ to be filled ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regression \n",
    "\n",
    "> SST = SSE + SSR \n",
    "> [ to be filled ] \n",
    ">\n",
    "> F-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
